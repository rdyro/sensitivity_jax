{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Optimization Sensitivity in JAX - Reference Documentation - First- and Second-order Optimization","text":"<p><code>sensitivity_jax</code> is a package designed to allow taking first- and second-order derivatives through optimization or any other fixed-point process.</p> <p>Source code for this package is located here: github.com/rdyro/sensitivity_jax.</p> <p>This package builds on top of JAX. We also maintain an implementation in PyTorch here.</p> <p> </p>"},{"location":"installation/","title":"Installation","text":"<p>Install using pip <pre><code>$ pip install git+https://github.com/rdyro/sensitivity_jax.git\n</code></pre> or from source <pre><code>$ git clone git@github.com:rdyro/sensitivity_jax.git\n$ cd sensitivity_jax\n$ python3 setup.py install --user\n</code></pre></p>"},{"location":"installation/#testing","title":"Testing","text":"<p>Run all unit tests using <pre><code>$ python3 setup.py test\n</code></pre></p>"},{"location":"tour/","title":"Tour of <code>sensitivity_jax</code>","text":"<pre><code>import jax, numpy as np\nfrom jax import numpy as jnp\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_moons\nfrom sensitivity_jax.sensitivity import (\nimplicit_jacobian,\ngenerate_optimization_fns,\n)\nfrom sensitivity_jax.extras.optimization.sqp import (\nminimize_agd,\nminimize_lbfgs,\nminimize_sqp,\n)\ndef feature_map(X, P, q):\nPhi = jnp.cos(X @ P + q)\nreturn Phi\ndef optimize_fn(P, q, X, Y):\n\"\"\"Non-differentiable optimization function using sklearn.\"\"\"\nPhi = feature_map(X, P, q)\nmodel = LogisticRegression(max_iter=300, C=1e-2)\nmodel.fit(Phi, Y)\nz = np.concatenate([model.coef_.reshape(-1), model.intercept_])\nreturn jnp.array(z)\ndef loss_fn(z, P, q, X, Y):\n\"\"\"The measure of model performance.\"\"\"\nPhi = feature_map(X, P, q)\nW, b = z[:-1], z[-1]\nprob = jax.nn.sigmoid(Phi @ W + b)\nloss = jnp.sum(-(Y * jnp.log(prob) + (1 - Y) * jnp.log(1 - prob)))\nregularization = 0.5 * jnp.sum(W ** 2) / 1e-2\nreturn (loss + regularization) / X.shape[-2]\ndef k_fn(z, P, q, X, Y):\n\"\"\"Optimalty conditions of the model \u2013 the fixed-point of optimization.\"\"\"\nreturn jax.grad(loss_fn)(z, P, q, X, Y)\nif __name__ == \"__main__\":\n######################## SETUP ####################################\n# generate data and split into train and test sets\nX, Y = make_moons(n_samples=1000, noise=1e-1)\nX, Y = jnp.array(X), jnp.array(Y)\nn_train = 200\nXtr, Ytr = X[:n_train, :], Y[:n_train]\nXts, Yts = X[n_train:, :], Y[n_train:]\n# generate the Fourier feature map parameters Phi = cos(X @ P + q)\nn_features = 200\nP = jnp.array(np.random.randn(2, n_features))\nq = jnp.array(np.zeros(n_features))\n# check that the fixed-point is numerically close to zero\nz = optimize_fn(P, q, X, Y)\nk = k_fn(z, P, q, X, Y)\nassert jnp.max(jnp.abs(k)) &lt; 1e-5\n######################## SENSITIVITY ##############################\n# generate sensitivity Jacobians and check their shape\nJP, Jq = implicit_jacobian(lambda z, P, q: k_fn(z, P, q, X, Y), z, P, q)\nassert JP.shape == z.shape + P.shape\nassert Jq.shape == z.shape + q.shape\n######################## OPTIMIZATION #############################\n# generate necessary functions for optimization\nopt_fn_ = lambda P: optimize_fn(P, q, Xtr, Ytr) # model optimization\nk_fn_ = lambda z, P: k_fn(z, P, q, Xtr, Ytr) # fixed-point\nloss_fn_ = lambda z, P: loss_fn(z, P, q, Xts, Yts) # loss to improve\nf_fn, g_fn, h_fn = generate_optimization_fns(loss_fn_, opt_fn_, k_fn_)\n# choose any optimization routine\n# Ps = minimize_agd(f_fn, g_fn, P, verbose=True, ai=1e-1, af=1e-1, max_it=100)\n# Ps = minimize_lbfgs(f_fn, g_fn, P, verbose=True, lr=1e-1, max_it=10)\nPs = minimize_sqp(f_fn, g_fn, h_fn, P, verbose=True, max_it=10)\ndef predict(z, P, q, X):\nPhi = feature_map(X, P, q)\nW, b = z[:-1], z[-1]\nprob = jax.nn.sigmoid(Phi @ W + b)\nreturn jnp.round(prob)\n# evaluate the results\nacc0 = jnp.mean(predict(opt_fn_(P), P, q, Xts) == Yts)\naccf = jnp.mean(predict(opt_fn_(Ps), Ps, q, Xts) == Yts)\nprint(\"Accuracy before: %4.2f%%\" % (1e2 * acc0))\nprint(\"Accuracy after:  %4.2f%%\" % (1e2 * accf))\nprint(\"Loss before:     %9.4e\" % loss_fn(opt_fn_(P), P, q, Xts, Yts))\nprint(\"Loss after:      %9.4e\" % loss_fn(opt_fn_(Ps), Ps, q, Xts, Yts))\n</code></pre>"},{"location":"api/overview/","title":"<code>sensitivity</code>","text":"name summary generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad, optimizations, jit, custom_arg_serializer) Directly generates upper/outer bilevel program derivative functions. generate_optimization_with_state_fns(loss_fn, opt_fn, k_fn, normalize_grad, optimizations, jit, custom_arg_serializer) Directly generates upper/outer bilevel program derivative functions. implicit_hessian(k_fn, z, params, nondiff_kw, Dg, Hg, jvp_vec, optimizations) Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec. implicit_jacobian(k_fn, z, params, nondiff_kw, Dg, jvp_vec, matrix_free_inverse, full_output, optimizations) Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec."},{"location":"api/overview/#batch_sensitivity","title":"<code>batch_sensitivity</code>","text":"name summary generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad, optimizations, jit, use_cache, kw_in_key) Directly generates upper/outer bilevel program derivative functions. implicit_hessian(k_fn, z, params, nondiff_kw, Dg, Hg, jvp_vec, optimizations) Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec, using batched operations. implicit_hessian2(k_fn, z, params, nondiff_kw, Dg, Hg, jvp_vec, optimizations) Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec, using vmap. implicit_jacobian(k_fn, z, params, nondiff_kw, Dg, jvp_vec, matrix_free_inverse, full_output, optimizations) Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec, using batched operations. implicit_jacobian2(k_fn, z, params, nondiff_kw, Dg, jvp_vec, matrix_free_inverse, full_output, optimizations) Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec, using vmap."},{"location":"api/overview/#extrasoptimizationagd","title":"<code>extras.optimization.agd</code>","text":"name summary minimize_agd(f_fn, g_fn, args, verbose, verbose_prefix, max_it, ai, af, full_output, callback_fn, use_writer, use_tqdm, state, optimizer) Minimize a loss function <code>f_fn</code> with Accelerated Gradient Descent (AGD) with respect to <code>*args</code>. Uses PyTorch."},{"location":"api/overview/#extrasoptimizationlbfgs","title":"<code>extras.optimization.lbfgs</code>","text":"name summary minimize_lbfgs(f_fn, g_fn, args, verbose, verbose_prefix, lr, max_it, full_output, callback_fn, use_writer, use_tqdm, state) Minimize a loss function <code>f_fn</code> with L-BFGS with respect to <code>*args</code>. Taken from PyTorch."},{"location":"api/overview/#extrasoptimizationsqp","title":"<code>extras.optimization.sqp</code>","text":"name summary minimize_sqp(f_fn, g_fn, h_fn, args, reg0, verbose, verbose_prefix, max_it, ls_pts_nb, force_step, full_output, callback_fn, use_writer, use_tqdm, state, parallel_ls) Minimizes an unconstrained objective using Sequential Quadratic Programming (SQP)."},{"location":"api/overview/#utils","title":"<code>utils</code>","text":"name summary fn_with_sol_and_state_cache(fwd_fn, cache, jit, use_cache, kw_in_key, custom_arg_serializer) Wraps a function in a version where computation of the first argument via fwd_fn is cached. fn_with_sol_cache(fwd_fn, cache, jit, use_cache, kw_in_key, custom_arg_serializer) Wraps a function in a version where computation of the first argument via fwd_fn is cached."},{"location":"api/overview/#differentiation","title":"<code>differentiation</code>","text":"name summary BATCH_HESSIAN(fn, config) Computes the Hessian, assuming the first in/out dimension is the batch. BATCH_JACOBIAN(fn, config) Computes the Hessian, assuming the first in/out dimension is the batch. HESSIAN_DIAG(fn, config) Generates a function which computes per-argument partial Hessians."},{"location":"api/sensitivity_jax/batch_sensitivity/generate_optimization_fns/","title":"Generate optimization fns","text":"next &gt;&gt;&gt;<p>sensitivity_jax.implicit_hessian</p>"},{"location":"api/sensitivity_jax/batch_sensitivity/generate_optimization_fns/#sensitivity_jax.batch_sensitivity.generate_optimization_fns","title":"<code>sensitivity_jax.batch_sensitivity.generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad=False, optimizations=None, jit=True, use_cache=True, kw_in_key=True)</code>","text":"<p>Directly generates upper/outer bilevel program derivative functions.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable</code> <p>loss_fn(z, *params), upper/outer level loss</p> required <code>opt_fn</code> <code>Callable</code> <p>opt_fn(*params) = z, lower/inner argmin function</p> required <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>normalize_grad</code> <code>bool</code> <p>whether to normalize the gradient by its norm</p> <code>False</code> <code>jit</code> <code>bool</code> <p>whether to apply just-in-time (jit) compilation to the functions</p> <code>True</code> <code>cache_solutions</code> <p>whether to cache the solution</p> required <p>Returns:</p> Type Description <p><code>f_fn(*params), g_fn(*params), h_fn(*params)</code></p> <p>parameters-only upper/outer level loss, gradient and Hessian.</p> Source code in <code>sensitivity_jax/batch_sensitivity.py</code> <pre><code>def generate_optimization_fns(\nloss_fn: Callable,\nopt_fn: Callable,\nk_fn: Callable,\nnormalize_grad: bool = False,\noptimizations: Mapping = None,\njit: bool = True,\nuse_cache: bool = True,\nkw_in_key: bool = True,\n):\n\"\"\"Directly generates upper/outer bilevel program derivative functions.\n    Args:\n        loss_fn: loss_fn(z, *params), upper/outer level loss\n        opt_fn: opt_fn(*params) = z, lower/inner argmin function\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        normalize_grad: whether to normalize the gradient by its norm\n        jit: whether to apply just-in-time (jit) compilation to the functions\n        cache_solutions: whether to cache the solution\n    Returns:\n        ``f_fn(*params), g_fn(*params), h_fn(*params)``\n        parameters-only upper/outer level loss, gradient and Hessian.\n    \"\"\"\nsol_cache = dict()\noptimizations = dict() if optimizations is None else copy(optimizations)\n@fn_with_sol_cache(opt_fn, sol_cache, jit=jit, use_cache=use_cache, kw_in_key=kw_in_key)\ndef f_fn(z, *params, **nondiff_kw):\nreturn loss_fn(z, *params, **nondiff_kw)\n@fn_with_sol_cache(opt_fn, sol_cache, jit=jit, use_cache=use_cache, kw_in_key=kw_in_key)\ndef g_fn(z, *params, **nondiff_kw):\ng = JACOBIAN(loss_fn, argnums=range(len(params) + 1))(\nz, *params, **nondiff_kw\n)\nDp = implicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\nDg=g[0],\noptimizations=optimizations,\n)\nDp = Dp if len(params) != 1 else [Dp]\nret = [Dp + g for (Dp, g) in zip(Dp, g[1:])]\nif normalize_grad:\nret = [(z / (jaxm.norm(z) + 1e-7)) for z in ret]\nreturn ret[0] if len(ret) == 1 else ret\n@fn_with_sol_cache(opt_fn, sol_cache, jit=jit, use_cache=use_cache, kw_in_key=kw_in_key)\ndef h_fn(z, *params, **nondiff_kw):\ng = JACOBIAN(loss_fn, argnums=range(len(params) + 1))(\nz, *params, **nondiff_kw\n)\nif optimizations.get(\"Hz_fn\", None) is None:\nloss_fn_ = lambda z, *params: loss_fn(z, *params, **nondiff_kw)\noptimizations[\"Hz_fn\"] = BATCH_HESSIAN(loss_fn_)\nHz_fn = optimizations[\"Hz_fn\"]\nHz = Hz_fn(z, *params)\nH = [Hz] + list(\nBATCH_HESSIAN_DIAG(\nlambda *params: loss_fn(z, *params, **nondiff_kw)\n)(*params)\n)\n_, Dpp = implicit_hessian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\nDg=g[0],\nHg=H[0],\noptimizations=optimizations,\n)\nDpp = Dpp if len(params) != 1 else [Dpp]\nret = [Dpp + H for (Dpp, H) in zip(Dpp, H[1:])]\nreturn ret[0] if len(ret) == 1 else ret\nreturn (f_fn, g_fn, h_fn)\n</code></pre>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_hessian/","title":"Implicit hessian","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.generate_optimization_fns</p>next &gt;&gt;&gt;<p>sensitivity_jax.implicit_hessian2</p>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_hessian/#sensitivity_jax.batch_sensitivity.implicit_hessian","title":"<code>sensitivity_jax.batch_sensitivity.implicit_hessian(k_fn, z, *params, nondiff_kw=None, Dg=None, Hg=None, jvp_vec=None, optimizations=None)</code>","text":"<p>Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec, using batched operations.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>JAXArray</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>JAXArray</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>nondiff_kw</code> <code>Mapping</code> <p>nondifferentiable parameters to the implicit function</p> <code>None</code> <code>Dg</code> <code>JAXArray</code> <p>gradient sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>Hg</code> <code>JAXArray</code> <p>Hessian sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>jvp_vec</code> <code>Union[JAXArray, Sequence[JAXArray]]</code> <p>right sensitivity vector(s) (wrt p) for Hessian-vector-product</p> <code>None</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Hessian/chain rule Hessian as specified by arguments</p> Source code in <code>sensitivity_jax/batch_sensitivity.py</code> <pre><code>def implicit_hessian(\nk_fn: Callable,\nz: JAXArray,\n*params: JAXArray,\nnondiff_kw: Mapping = None,\nDg: JAXArray = None,\nHg: JAXArray = None,\njvp_vec: Union[JAXArray, Sequence[JAXArray]] = None,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Hessian or chain rule depending on Dg, Hg,\n    jvp_vec, using batched operations.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        nondiff_kw: nondifferentiable parameters to the implicit function\n        Dg: gradient sensitivity vector (wrt z), for chain rule\n        Hg: Hessian sensitivity vector (wrt z), for chain rule\n        jvp_vec: right sensitivity vector(s) (wrt p) for Hessian-vector-product\n        optimizations: optional optimizations\n    Returns:\n        Hessian/chain rule Hessian as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nnondiff_kw = {} if nondiff_kw is None else nondiff_kw\nblen, zlen = z.shape[0], prod(z.shape[1:])\nplen = [prod(param.shape[1:]) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\nif jvp_vec is not None:\nassert Dg is not None\nif nondiff_kw is not None:\nk_fn_ = lambda z, *params: k_fn(z, *params, **nondiff_kw)\nelse:\nk_fn_ = k_fn\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn_)\n#############################################################\n# compute 2nd implicit gradients\nif Dg is not None:\nassert Dg.size == zlen * blen\nassert Hg is None or Hg.size == blen * zlen ** 2\nDg_ = Dg.reshape((blen, zlen, 1))\nHg_ = Hg.reshape((blen, zlen, zlen)) if Hg is not None else Hg\n# compute the left hand vector in the VJP\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(z, *params, rhs=Dg_.reshape((blen, zlen, 1)), T=True)\nfn = lambda z, *params: jaxm.sum(\nv.reshape((blen, zlen)) * k_fn_(z, *params).reshape((blen, zlen))\n)\nif jvp_vec is not None:\nDpz_jvp = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\njvp_vec=jvp_vec,\noptimizations=optimizations,\n)\n)\nDpz_jvp = [Dpz_jvp.reshape(-1) for Dpz_jvp in Dpz_jvp]\n# compute the 2nd order derivatives consisting of 4 terms\n# term 1 ##############################\ndfn_params = jaxm.grad(\nlambda *params: fn(z, *params), argnums=range(len(params))\n)\nDpp1 = _ensure_list(jaxm.jvp(dfn_params, params, tuple(jvp_vec))[1])\nDpp1 = [\nDpp1.reshape((blen, plen)) for (Dpp1, plen) in zip(Dpp1, plen)\n]\n# term 2 ##############################\nDpp2 = [\njaxm.jvp(\nlambda z: jaxm.grad(fn, argnums=i + 1)(z, *params),\n(z,),\n(Dpz_jvp.reshape(z.shape),),\n)[1].reshape((blen, plen))\nfor (i, (Dpz_jvp, plen)) in enumerate(zip(Dpz_jvp, plen))\n]\n# term 3 ##############################\ng_ = _ensure_list(\njaxm.jvp(\nlambda *params: jaxm.grad(fn)(z, *params),\nparams,\ntuple(jvp_vec),\n)[1]\n)\nDpp3 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\nDg=g_,\noptimizations=optimizations,\n)\n)[i].reshape((blen, plen))\nfor (i, (g_, plen)) in enumerate(zip(g_, plen))\n]\n# term 4 ##############################\ng_ = [\njaxm.jvp(\nlambda z: jaxm.grad(fn)(z, *params),\n(z,),\n(Dpz_jvp.reshape(z.shape),),\n)[1]\nfor (i, Dpz_jvp) in enumerate(Dpz_jvp)\n]\nif Hg is not None:\ng_ = [\ng_.reshape((blen, zlen))\n+ bmv(Hg_, Dpz_jvp.reshape((blen, zlen)))\nfor (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n]\nDpp4 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nDg=g_,\nnondiff_kw=nondiff_kw,\noptimizations=optimizations,\n)\n)[i].reshape((blen, plen))\nfor (i, (g_, plen)) in enumerate(zip(g_, plen))\n]\nDp = [\njaxm.sum(\nDg_.reshape((blen, zlen)) * Dpz_jvp.reshape((blen, zlen)),\n-1,\n)\nfor Dpz_jvp in Dpz_jvp\n]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [Dp.reshape((blen,)) for Dp in Dp]\nDpp_shaped = [\nDpp.reshape(param.shape) for (Dpp, param) in zip(Dpp, params)\n]\nelse:\n# compute the full first order 1st gradients\nDpz = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\noptimizations=optimizations,\n)\n)\nDpz = [\nDpz.reshape((blen, zlen, plen))\nfor (Dpz, plen) in zip(Dpz, plen)\n]\n# compute the 2nd order derivatives consisting of 4 terms\nDpp1 = BATCH_HESSIAN_DIAG(lambda *params: fn(z, *params))(*params)\nDpp1 = [\nDpp1.reshape((blen, plen, plen))\nfor (Dpp1, plen) in zip(Dpp1, plen)\n]\ntemp = BATCH_JACOBIAN(\nlambda *params: jaxm.grad(fn)(z, *params),\nargnums=range(len(params)),\n)(*params)\ntemp = [\njaxm.t(temp.reshape((blen, zlen, plen)))\nfor (temp, plen) in zip(temp, plen)\n]\nDpp2 = [\n(temp @ Dpz).reshape((blen, plen, plen))\nfor (temp, Dpz, plen) in zip(temp, Dpz, plen)\n]\nDpp3 = [jaxm.t(Dpp2) for Dpp2 in Dpp2]\nDzz = BATCH_HESSIAN(lambda z: fn(z, *params))(z).reshape(\n(blen, zlen, zlen)\n)\nif Hg is not None:\nDpp4 = [jaxm.t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\nelse:\nDpp4 = [jaxm.t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\nDp = [Dg_.reshape((blen, 1, zlen)) @ Dpz for Dpz in Dpz]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [\nDp.reshape(param.shape) for (Dp, param) in zip(Dp, params)\n]\nDpp_shaped = [\nDpp.reshape((blen,) + 2 * param.shape[1:])\nfor (Dpp, param) in zip(Dpp, params)\n]\nreturn (\n(Dp_shaped[0], Dpp_shaped[0])\nif len(params) == 1\nelse (Dp_shaped, Dpp_shaped)\n)\nelse:\nDpz, optimizations = implicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\nfull_output=True,\noptimizations=optimizations,\n)\nDpz = _ensure_list(Dpz)\nDpz = [\nDpz.reshape((blen, zlen, plen)) for (Dpz, plen) in zip(Dpz, plen)\n]\n# compute derivatives\nif optimizations.get(\"Dzzk\", None) is None:\nHk = BATCH_HESSIAN_DIAG(k_fn_)(z, *params)\nDzzk, Dppk = Hk[0], Hk[1:]\noptimizations[\"Dzzk\"] = Dzzk\nelse:\nDppk = BATCH_HESSIAN_DIAG(lambda *params: k_fn_(z, *params))(\n*params\n)\nDppk = [\nDppk.reshape((blen, zlen, plen, plen))\nfor (Dppk, plen) in zip(Dppk, plen)\n]\nDzpk = BATCH_JACOBIAN(\nlambda *params: BATCH_JACOBIAN(k_fn_)(z, *params),\nargnums=range(len(params)),\n)(*params)\nDzzk = Dzzk.reshape((blen, zlen, zlen, zlen))\nDzpk = [\nDzpk.reshape((blen, zlen, zlen, plen))\nfor (Dzpk, plen) in zip(Dzpk, plen)\n]\nDpzk = [jaxm.t(Dzpk) for Dzpk in Dzpk]\n# solve the IFT equation\nlhs = [\nDppk\n+ Dpzk @ Dpz[:, None, ...]\n+ jaxm.t(Dpz)[:, None, ...] @ Dzpk\n+ (jaxm.t(Dpz)[:, None, ...] @ Dzzk) @ Dpz[:, None, ...]\nfor (Dpz, Dzpk, Dpzk, Dppk) in zip(Dpz, Dzpk, Dpzk, Dppk)\n]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDppz = [\n-Dzk_solve_fn(\nz, *params, rhs=lhs.reshape((blen, zlen, plen * plen)), T=False\n).reshape((blen, zlen, plen, plen))\nfor (lhs, plen) in zip(lhs, plen)\n]\n# return computed values\nDpz_shaped = [\nDpz.reshape((blen,) + z.shape[1:] + param.shape[1:])\nfor (Dpz, param) in zip(Dpz, params)\n]\nDppz_shaped = [\nDppz.reshape((blen,) + z.shape[1:] + 2 * param.shape[1:])\nfor (Dppz, param) in zip(Dppz, params)\n]\nreturn (\n(Dpz_shaped[0], Dppz_shaped[0])\nif len(params) == 1\nelse (Dpz_shaped, Dppz_shaped)\n)\n</code></pre>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_hessian2/","title":"Implicit hessian2","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.implicit_hessian</p>next &gt;&gt;&gt;<p>sensitivity_jax.implicit_jacobian</p>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_hessian2/#sensitivity_jax.batch_sensitivity.implicit_hessian2","title":"<code>sensitivity_jax.batch_sensitivity.implicit_hessian2(k_fn, z, *params, nondiff_kw=None, Dg=None, Hg=None, jvp_vec=None, optimizations=None)</code>","text":"<p>Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec, using vmap.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>JAXArray</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>JAXArray</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>JAXArray</code> <p>gradient sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>Hg</code> <code>JAXArray</code> <p>Hessian sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>jvp_vec</code> <code>Union[JAXArray, Sequence[JAXArray]]</code> <p>right sensitivity vector(s) (wrt p) for Hessian-vector-product</p> <code>None</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Hessian/chain rule Hessian as specified by arguments</p> Source code in <code>sensitivity_jax/batch_sensitivity.py</code> <pre><code>def implicit_hessian2(\nk_fn: Callable,\nz: JAXArray,\n*params: JAXArray,\nnondiff_kw: Mapping = None,\nDg: JAXArray = None,\nHg: JAXArray = None,\njvp_vec: Union[JAXArray, Sequence[JAXArray]] = None,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec,\n    using vmap.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: gradient sensitivity vector (wrt z), for chain rule\n        Hg: Hessian sensitivity vector (wrt z), for chain rule\n        jvp_vec: right sensitivity vector(s) (wrt p) for Hessian-vector-product\n        optimizations: optional optimizations\n    Returns:\n        Hessian/chain rule Hessian as specified by arguments\n    \"\"\"\n# we need a custom rule for optimizations\noptimizations = {} if optimizations is None else optimizations\nnondiff_kw = {} if nondiff_kw is None else nondiff_kw\noptimizations = _split_for_broadcast(optimizations, z.shape[0])\nnondiff_kw = _split_for_broadcast(nondiff_kw, z.shape[0])\n# call the function with vmap\nreturn jaxm.vmap(\nlambda z, *params, nondiff_kw_=None, Dg=None, Hg=None, jvp_vec=None, optimizations_=None: implicit_hessian_(\nk_fn,\nz,\n*params,\nnondiff_kw=None\nif nondiff_kw is None\nelse dict(nondiff_kw_, **nondiff_kw[1]),\nDg=Dg,\nHg=Hg,\njvp_vec=jvp_vec,\noptimizations=None\nif optimizations is None\nelse dict(optimizations_, **optimizations[1]),\n)\n)(\nz,\n*params,\nnondiff_kw_=None if nondiff_kw is None else nondiff_kw[0],\nDg=Dg,\nHg=Hg,\njvp_vec=jvp_vec,\noptimizations_=None if optimizations is None else optimizations[0],\n)\n</code></pre>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_jacobian/","title":"Implicit jacobian","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.implicit_hessian2</p>next &gt;&gt;&gt;<p>sensitivity_jax.implicit_jacobian2</p>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_jacobian/#sensitivity_jax.batch_sensitivity.implicit_jacobian","title":"<code>sensitivity_jax.batch_sensitivity.implicit_jacobian(k_fn, z, *params, nondiff_kw=None, Dg=None, jvp_vec=None, matrix_free_inverse=False, full_output=False, optimizations=None)</code>","text":"<p>Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec, using batched operations.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>JAXArray</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>JAXArray</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>nondiff_kw</code> <code>Mapping</code> <p>nondifferentiable parameters to the implicit function</p> <code>None</code> <code>Dg</code> <code>JAXArray</code> <p>left sensitivity vector (wrt z), for a VJP</p> <code>None</code> <code>jvp_vec</code> <code>Union[JAXArray, Sequence[JAXArray]]</code> <p>right sensitivity vector(s) (wrt p) for a JVP</p> <code>None</code> <code>matrix_free_inverse</code> <code>bool</code> <p>whether to use approximate matrix inversion</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to append accumulated optimizations to the output</p> <code>False</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Jacobian/VJP/JVP as specified by arguments</p> Source code in <code>sensitivity_jax/batch_sensitivity.py</code> <pre><code>def implicit_jacobian(\nk_fn: Callable,\nz: JAXArray,\n*params: JAXArray,\nnondiff_kw: Mapping = None,\nDg: JAXArray = None,\njvp_vec: Union[JAXArray, Sequence[JAXArray]] = None,\nmatrix_free_inverse: bool = False,\nfull_output: bool = False,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec,\n    using batched operations.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        nondiff_kw: nondifferentiable parameters to the implicit function\n        Dg: left sensitivity vector (wrt z), for a VJP\n        jvp_vec: right sensitivity vector(s) (wrt p) for a JVP\n        matrix_free_inverse: whether to use approximate matrix inversion\n        full_output: whether to append accumulated optimizations to the output\n        optimizations: optional optimizations\n    Returns:\n        Jacobian/VJP/JVP as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nblen, zlen = z.shape[0], prod(z.shape[1:])\nplen = [prod(param.shape[1:]) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\nif nondiff_kw is not None:\nk_fn_ = lambda z, *params: k_fn(z, *params, **nondiff_kw)\nelse:\nk_fn_ = k_fn\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn_)\n#############################################################\nif Dg is not None:\nif matrix_free_inverse:\nraise NotImplementedError\nelse:\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(\nz, *params, rhs=Dg.reshape((blen, zlen, 1)), T=True\n)\nfn = lambda *params: jaxm.sum(\nv.reshape((blen, zlen)) * k_fn_(z, *params).reshape((blen, zlen))\n)\nDp = JACOBIAN(fn, argnums=range(len(params)))(*params)\nDp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\nret = Dp_shaped[0] if len(params) == 1 else Dp_shaped\nelse:\nif jvp_vec is not None:\nfn = lambda *params: k_fn_(z, *params)\nDp = _ensure_list(jaxm.jvp(fn, params, tuple(jvp_vec))[1])\nDp = [Dp.reshape((blen, zlen, 1)) for (Dp, plen) in zip(Dp, plen)]\nDpk = Dp\nelse:\nDpk = BATCH_JACOBIAN(\nlambda *params: k_fn_(z, *params),\nargnums=range(len(params)),\n)(*params)\nDpk = [\nDpk.reshape((blen, zlen, plen))\nfor (Dpk, plen) in zip(Dpk, plen)\n]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDpz = [-Dzk_solve_fn(z, *params, rhs=Dpk, T=False) for Dpk in Dpk]\nif jvp_vec is not None:\nDpz_shaped = [Dpz.reshape(z.shape) for Dpz in Dpz]\nelse:\nDpz_shaped = [\nDpz.reshape((blen,) + z.shape[1:] + param.shape[1:])\nfor (Dpz, param) in zip(Dpz, params)\n]\nret = Dpz_shaped if len(params) != 1 else Dpz_shaped[0]\nreturn (ret, optimizations) if full_output else ret\n</code></pre>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_jacobian2/","title":"Implicit jacobian2","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.implicit_jacobian</p>next &gt;&gt;&gt;<p>sensitivity_jax.BATCH_HESSIAN</p>"},{"location":"api/sensitivity_jax/batch_sensitivity/implicit_jacobian2/#sensitivity_jax.batch_sensitivity.implicit_jacobian2","title":"<code>sensitivity_jax.batch_sensitivity.implicit_jacobian2(k_fn, z, *params, nondiff_kw=None, Dg=None, jvp_vec=None, matrix_free_inverse=False, full_output=False, optimizations=None)</code>","text":"<p>Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec, using vmap.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>JAXArray</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>JAXArray</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>JAXArray</code> <p>left sensitivity vector (wrt z), for a VJP</p> <code>None</code> <code>jvp_vec</code> <code>Union[JAXArray, Sequence[JAXArray]]</code> <p>right sensitivity vector(s) (wrt p) for a JVP</p> <code>None</code> <code>matrix_free_inverse</code> <code>bool</code> <p>whether to use approximate matrix inversion</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to append accumulated optimizations to the output</p> <code>False</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Jacobian/VJP/JVP as specified by arguments</p> Source code in <code>sensitivity_jax/batch_sensitivity.py</code> <pre><code>def implicit_jacobian2(\nk_fn: Callable,\nz: JAXArray,\n*params: JAXArray,\nnondiff_kw: Mapping = None,\nDg: JAXArray = None,\njvp_vec: Union[JAXArray, Sequence[JAXArray]] = None,\nmatrix_free_inverse: bool = False,\nfull_output: bool = False,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec,\n    using vmap.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: left sensitivity vector (wrt z), for a VJP\n        jvp_vec: right sensitivity vector(s) (wrt p) for a JVP\n        matrix_free_inverse: whether to use approximate matrix inversion\n        full_output: whether to append accumulated optimizations to the output\n        optimizations: optional optimizations\n    Returns:\n        Jacobian/VJP/JVP as specified by arguments\n    \"\"\"\n# we need a custom rule for optimizations\noptimizations = {} if optimizations is None else optimizations\nnondiff_kw = {} if nondiff_kw is None else nondiff_kw\n# optimizations = _split_into_array_and_not(optimizations)\n# nondiff_kw = _split_into_array_and_not(nondiff_kw)\noptimizations = _split_for_broadcast(optimizations, z.shape[0])\nnondiff_kw = _split_for_broadcast(nondiff_kw, z.shape[0])\n# call the function with vmap\nreturn jaxm.vmap(\nlambda z, *params, nondiff_kw_=None, Dg=None, jvp_vec=None, optimizations_=None: implicit_jacobian_(\nk_fn,\nz,\n*params,\nnondiff_kw=None\nif nondiff_kw is None\nelse dict(nondiff_kw_, **nondiff_kw[1]),\nDg=Dg,\njvp_vec=jvp_vec,\noptimizations=None\nif optimizations is None\nelse dict(optimizations_, **optimizations[1]),\n)\n)(\nz,\n*params,\nnondiff_kw_=None if nondiff_kw is None else nondiff_kw[0],\nDg=Dg,\njvp_vec=jvp_vec,\noptimizations_=None if optimizations is None else optimizations[0],\n)\n</code></pre>"},{"location":"api/sensitivity_jax/differentiation/BATCH_HESSIAN/","title":"BATCH HESSIAN","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.implicit_jacobian2</p>next &gt;&gt;&gt;<p>sensitivity_jax.BATCH_JACOBIAN</p>"},{"location":"api/sensitivity_jax/differentiation/BATCH_HESSIAN/#sensitivity_jax.differentiation.BATCH_HESSIAN","title":"<code>sensitivity_jax.differentiation.BATCH_HESSIAN(fn, **config)</code>","text":"<p>Computes the Hessian, assuming the first in/out dimension is the batch.</p> Source code in <code>sensitivity_jax/differentiation.py</code> <pre><code>def BATCH_HESSIAN(fn, **config):\n\"\"\"Computes the Hessian, assuming the first in/out dimension is the batch.\"\"\"\ndef batch_hess(*args, **kw):\nreturn BATCH_JACOBIAN(BATCH_JACOBIAN(fn, **config), **config)(*args, **kw)\nreturn batch_hess\n</code></pre>"},{"location":"api/sensitivity_jax/differentiation/BATCH_JACOBIAN/","title":"BATCH JACOBIAN","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.BATCH_HESSIAN</p>next &gt;&gt;&gt;<p>sensitivity_jax.HESSIAN_DIAG</p>"},{"location":"api/sensitivity_jax/differentiation/BATCH_JACOBIAN/#sensitivity_jax.differentiation.BATCH_JACOBIAN","title":"<code>sensitivity_jax.differentiation.BATCH_JACOBIAN(fn, **config)</code>","text":"<p>Computes the Hessian, assuming the first in/out dimension is the batch.</p> Source code in <code>sensitivity_jax/differentiation.py</code> <pre><code>def BATCH_JACOBIAN(fn, **config):\n\"\"\"Computes the Hessian, assuming the first in/out dimension is the batch.\"\"\"\ndef batch_jac(*args, **kw):\nret = jaxm.jacobian(\nlambda *args, **kw: jaxm.sum(jaxm.atleast_1d(fn(*args, **kw)), 0), **config\n)(*args, **kw)\nargnums = config.get(\"argnums\", 0)\nargnums = list(argnums) if hasattr(argnums, \"__iter__\") else argnums\nJs, ret_struct = tree_flatten(ret)\nargnums, argnums_struct = tree_flatten(argnums)\nout_shapes = [J.shape[: -len(args[argnum].shape)] for (J, argnum) in zip(Js, argnums)]\nJs = [\nJ.reshape((prod(out_shape),) + args[argnum].shape)\n.swapaxes(0, 1)\n.reshape((args[argnum].shape[0],) + out_shape + args[argnum].shape[1:])\nfor (J, out_shape, argnum) in zip(Js, out_shapes, argnums)\n]\nret = tree_unflatten(ret_struct, Js)\nreturn ret\nreturn batch_jac\n</code></pre>"},{"location":"api/sensitivity_jax/differentiation/HESSIAN_DIAG/","title":"HESSIAN DIAG","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.BATCH_JACOBIAN</p>next &gt;&gt;&gt;<p>sensitivity_jax.extras.optimization.minimize_agd</p>"},{"location":"api/sensitivity_jax/differentiation/HESSIAN_DIAG/#sensitivity_jax.differentiation.HESSIAN_DIAG","title":"<code>sensitivity_jax.differentiation.HESSIAN_DIAG(fn, **config)</code>","text":"<p>Generates a function which computes per-argument partial Hessians.</p> Source code in <code>sensitivity_jax/differentiation.py</code> <pre><code>def HESSIAN_DIAG(fn, **config):\n\"\"\"Generates a function which computes per-argument partial Hessians.\"\"\"\ndef h_fn(*args, **kw):\nargs = (args,) if not isinstance(args, (tuple, list)) else tuple(args)\nret = [\njaxm.hessian(lambda arg: fn(*args[:i], arg, *args[i + 1 :], **kw), **config)(arg)\nfor (i, arg) in enumerate(args)\n]\nreturn ret\nreturn h_fn\n</code></pre>"},{"location":"api/sensitivity_jax/extras/optimization/agd/minimize_agd/","title":"Minimize agd","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.HESSIAN_DIAG</p>next &gt;&gt;&gt;<p>sensitivity_jax.extras.optimization.minimize_lbfgs</p>"},{"location":"api/sensitivity_jax/extras/optimization/agd/minimize_agd/#sensitivity_jax.extras.optimization.agd.minimize_agd","title":"<code>sensitivity_jax.extras.optimization.agd.minimize_agd(f_fn, g_fn, *args, verbose=False, verbose_prefix='', max_it=10 ** 3, ai=0.1, af=0.01, full_output=False, callback_fn=None, use_writer=False, use_tqdm=False, state=None, optimizer='Adam')</code>","text":"<p>Minimize a loss function <code>f_fn</code> with Accelerated Gradient Descent (AGD) with respect to <code>*args</code>. Uses PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>f_fn</code> <code>Callable</code> <p>loss function</p> required <code>g_fn</code> <code>Callable</code> <p>gradient of the loss function</p> required <code>*args</code> <code>JAXArray</code> <p>arguments to be optimized</p> <code>()</code> <code>verbose</code> <code>bool</code> <p>whether to print output</p> <code>False</code> <code>verbose_prefix</code> <code>str</code> <p>prefix to append to verbose output, e.g. indentation</p> <code>''</code> <code>max_it</code> <code>int</code> <p>maximum number of iterates</p> <code>10 ** 3</code> <code>ai</code> <code>float</code> <p>initial gradient step length (exponential schedule)</p> <code>0.1</code> <code>af</code> <code>float</code> <p>final gradient step length (exponential schedule)</p> <code>0.01</code> <code>full_output</code> <code>bool</code> <p>whether to output optimization history</p> <code>False</code> <code>callback_fn</code> <code>Callable</code> <p>callback function of the form <code>cb_fn(*args, **kw)</code></p> <code>None</code> <code>use_writer</code> <code>bool</code> <p>whether to use tensorflow's Summary Writer (via PyTorch)</p> <code>False</code> <code>use_tqdm</code> <code>Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook]</code> <p>whether to use tqdm (to estimate total runtime)</p> <code>False</code> <p>Returns:</p> Type Description <p>Optimized <code>args</code> or <code>(args, args_hist)</code> if <code>full_output</code> is <code>True</code></p> Source code in <code>sensitivity_jax/extras/optimization/agd.py</code> <pre><code>def minimize_agd(\nf_fn: Callable,\ng_fn: Callable,\n*args: JAXArray,\nverbose: bool = False,\nverbose_prefix: str = \"\",\nmax_it: int = 10**3,\nai: float = 1e-1,\naf: float = 1e-2,\nfull_output: bool = False,\ncallback_fn: Callable = None,\nuse_writer: bool = False,\nuse_tqdm: Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook] = False,\nstate: Optional[Dict[str, Any]] = None,\noptimizer: str = \"Adam\",\n):\n\"\"\"Minimize a loss function ``f_fn`` with Accelerated Gradient Descent (AGD)\n    with respect to ``*args``. Uses PyTorch.\n    Args:\n        f_fn: loss function\n        g_fn: gradient of the loss function\n        *args: arguments to be optimized\n        verbose: whether to print output\n        verbose_prefix: prefix to append to verbose output, e.g. indentation\n        max_it: maximum number of iterates\n        ai: initial gradient step length (exponential schedule)\n        af: final gradient step length (exponential schedule)\n        full_output: whether to output optimization history\n        callback_fn: callback function of the form ``cb_fn(*args, **kw)``\n        use_writer: whether to use tensorflow's Summary Writer (via PyTorch)\n        use_tqdm: whether to use tqdm (to estimate total runtime)\n    Returns:\n        Optimized ``args`` or ``(args, args_hist)`` if ``full_output`` is ``True``\n    \"\"\"\nstate = dict() if state is None else state\ndtype = args[0].dtype\ndevice = args[0].device() if hasattr(args[0], \"device\") else \"cpu\"\nif isinstance(use_tqdm, bool):\nif use_tqdm:\nprint_fn, rng_wrapper = tqdm_module.tqdm.write, tqdm_module.tqdm\nelse:\nprint_fn, rng_wrapper = print, lambda x: x\nelse:\nprint_fn, rng_wrapper = use_tqdm.write, use_tqdm\nassert len(args) &gt; 0\nargs = [x2t(arg).clone().detach() for arg in args] if USE_TORCH else list(args)\nimprv = float(\"inf\")\ngam = (af / ai) ** (1.0 / max_it)\nif USE_TORCH:\nopt = state.get(\"opt\", OPT_MAP.get(optimizer.lower(), DEFAULT_OPTIMIZER)(args, lr=ai))\nelse:\nopt = state.get(\"opt\", OPT_MAP.get(optimizer.lower(), DEFAULT_OPTIMIZER)(ai))\nopt_state = state.get(\"opt_state\", opt.init(args))\nopt_update_fn = jax.jit(opt.update)\ntp = TablePrinter(\n[\"it\", \"imprv\", \"loss\", \"||g||_2\"],\n[\"%05d\", \"%9.4e\", \"%9.4e\", \"%9.4e\"],\nprefix=verbose_prefix,\nuse_writer=use_writer,\n)\nargs_hist = [[arg.detach().clone() for arg in args]] if USE_TORCH else [[arg for arg in args]]\nif callback_fn is not None:\nif USE_TORCH:\ncallback_fn(*[t2j(arg) for arg in args], opt=opt)\nelse:\ncallback_fn(*args, opt=opt, opt_state=opt_state)\nif verbose:\nprint_fn(tp.make_header())\ntry:\nfor it in rng_wrapper(range(max_it)):\nif USE_TORCH:\nargs_prev = [arg.clone().detach() for arg in args]\nopt.zero_grad()\nargs_ = [jaxm.to(t2j(arg), device=device, dtype=dtype) for arg in args]\nloss = torch.mean(x2t(f_fn(*args_)))\ngs = g_fn(*args_)\ngs = gs if isinstance(gs, list) or isinstance(gs, tuple) else [gs]\ngs = [x2t(g) for g in gs]\nfor arg, g in zip(args, gs):\narg.grad = torch.detach(g)\ng_norm = sum(\ntorch.norm(arg.grad) for arg in args if arg.grad is not None\n).detach() / len(args)\nopt.step()\nargs_hist.append([arg.detach().clone() for arg in args])\nif callback_fn is not None:\ncallback_fn(*[t2j(arg) for arg in args], opt=opt)\nimprv = sum(torch.norm(arg_prev - arg) for (arg, arg_prev) in zip(args, args_prev))\nimprv, loss = imprv.detach(), loss.detach()\nelse:\nargs_prev = args\nloss = jaxm.mean(f_fn(*args))\ngs = g_fn(*args)\ngs = gs if isinstance(gs, list) or isinstance(gs, tuple) else [gs]\ng_norm = sum(jaxm.norm(g) for g in gs) / len(gs)\nupdates, opt_state = opt_update_fn([ai * gam**it * g for g in gs], opt_state)\nargs = apply_updates(args, updates)\nargs_hist.append(args)\nif callback_fn is not None:\ncallback_fn(*[t2j(arg) for arg in args], opt=opt, opt_state=opt_state)\nimprv = sum(jaxm.norm(arg_prev - arg) for (arg, arg_prev) in zip(args, args_prev))\nif verbose or use_writer:\nline = tp.make_values([it, imprv, loss, g_norm])\nif verbose:\nprint_fn(line)\nif USE_TORCH:\nfor pgroup in opt.param_groups:\npgroup[\"lr\"] *= gam\nexcept KeyboardInterrupt:\npass\nif verbose:\nprint_fn(tp.make_footer())\nif USE_TORCH:\nret = [t2j(arg.detach()) for arg in args]\nret = ret if len(args) &gt; 1 else ret[0]\nargs_hist = [[t2j(arg) for arg in z] for z in args_hist]\nargs_hist = [z if len(args) &gt; 1 else z[0] for z in args_hist]\nstate = dict(state, opt=opt)\nelse:\nret = args if len(args) &gt; 1 else args[0]\nargs_hist = [z if len(args) &gt; 1 else z[0] for z in args_hist]\nstate = dict(state, opt=opt, opt_state=opt_state)\nreturn (ret, args_hist, state) if full_output else ret\n</code></pre>"},{"location":"api/sensitivity_jax/extras/optimization/lbfgs/minimize_lbfgs/","title":"Minimize lbfgs","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.extras.optimization.minimize_agd</p>next &gt;&gt;&gt;<p>sensitivity_jax.extras.optimization.minimize_sqp</p>"},{"location":"api/sensitivity_jax/extras/optimization/lbfgs/minimize_lbfgs/#sensitivity_jax.extras.optimization.lbfgs.minimize_lbfgs","title":"<code>sensitivity_jax.extras.optimization.lbfgs.minimize_lbfgs(f_fn, g_fn, *args, verbose=False, verbose_prefix='', lr=1.0, max_it=100, full_output=False, callback_fn=None, use_writer=False, use_tqdm=False, state=None)</code>","text":"<p>Minimize a loss function <code>f_fn</code> with L-BFGS with respect to <code>*args</code>. Taken from PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>f_fn</code> <code>Callable</code> <p>loss function</p> required <code>g_fn</code> <code>Callable</code> <p>gradient of the loss function</p> required <code>*args</code> <code>JAXArray</code> <p>arguments to be optimized</p> <code>()</code> <code>verbose</code> <code>bool</code> <p>whether to print output</p> <code>False</code> <code>verbose_prefix</code> <code>str</code> <p>prefix to append to verbose output, e.g. indentation</p> <code>''</code> <code>lr</code> <code>float</code> <p>learning rate, where 1.0 is unstable, use 1e-1 in most cases</p> <code>1.0</code> <code>max_it</code> <code>int</code> <p>maximum number of iterates</p> <code>100</code> <code>full_output</code> <code>bool</code> <p>whether to output optimization history</p> <code>False</code> <code>callback_fn</code> <code>Callable</code> <p>callback function of the form <code>cb_fn(*args, **kw)</code></p> <code>None</code> <code>use_writer</code> <code>bool</code> <p>whether to use tensorflow's Summary Writer (via PyTorch)</p> <code>False</code> <code>use_tqdm</code> <code>Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook]</code> <p>whether to use tqdm (to estimate total runtime)</p> <code>False</code> <p>Returns:</p> Type Description <p>Optimized <code>args</code> or <code>(args, args_hist, state)</code> if <code>full_output</code> is <code>True</code></p> Source code in <code>sensitivity_jax/extras/optimization/lbfgs.py</code> <pre><code>def minimize_lbfgs(\nf_fn: Callable,\ng_fn: Callable,\n*args: JAXArray,\nverbose: bool = False,\nverbose_prefix: str = \"\",\nlr: float = 1e0,\nmax_it: int = 100,\nfull_output: bool = False,\ncallback_fn: Callable = None,\nuse_writer: bool = False,\nuse_tqdm: Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook] = False,\nstate: Optional[Dict[str, Any]] = None,\n):\n\"\"\"Minimize a loss function `f_fn` with L-BFGS with respect to `*args`.\n    Taken from PyTorch.\n    Args:\n        f_fn: loss function\n        g_fn: gradient of the loss function\n        *args: arguments to be optimized\n        verbose: whether to print output\n        verbose_prefix: prefix to append to verbose output, e.g. indentation\n        lr: learning rate, where 1.0 is unstable, use 1e-1 in most cases\n        max_it: maximum number of iterates\n        full_output: whether to output optimization history\n        callback_fn: callback function of the form ``cb_fn(*args, **kw)``\n        use_writer: whether to use tensorflow's Summary Writer (via PyTorch)\n        use_tqdm: whether to use tqdm (to estimate total runtime)\n    Returns:\n        Optimized `args` or `(args, args_hist, state)` if `full_output` is `True`\n    \"\"\"\nstate = dict() if state is None else state\ndtype = args[0].dtype\ndevice = args[0].device() if hasattr(args[0], \"device\") else \"cpu\"\nif isinstance(use_tqdm, bool):\nif use_tqdm:\nprint_fn, rng_wrapper = tqdm_module.tqdm.write, tqdm_module.tqdm\nelse:\nprint_fn, rng_wrapper = print, lambda x: x\nelse:\nprint_fn, rng_wrapper = use_tqdm.write, use_tqdm\nassert len(args) &gt; 0\nargs = [x2t(arg).clone().detach() for arg in args] if USE_TORCH else args\nit, imprv = 0, float(\"inf\")\nif USE_TORCH:\nopt = state.get(\"opt\", torch.optim.LBFGS(args, lr=lr))\nelse:\nassert len(args) == 1, \"`jaxopt.LBFGS` only supports one argument\"\narg = args[0]\n@jax.jit\ndef value_and_grad_fn(x):\nreturn lr * f_fn(x), lr * g_fn(x)\nopt = jaxopt.LBFGS(value_and_grad_fn, value_and_grad=True, jit=True)\nopt_state = opt.init_state(arg)\nopt_update_fn = jax.jit(opt.update)\nargs_hist = [[arg.detach().clone() for arg in args]] if USE_TORCH else [arg]\nif callback_fn is not None:\nif USE_TORCH:\ncallback_fn(*[t2j(arg) for arg in args], opt=opt)\nelse:\ncallback_fn(*[t2j(arg) for arg in args], opt=opt, opt_state=opt_state)\nif USE_TORCH:\ndef closure():\nopt.zero_grad()\nargs_ = [t2j(arg) for arg in args]\nargs_ = [jaxm.to(t2j(arg), device=device, dtype=dtype) for arg in args]\nloss = torch.mean(x2t(f_fn(*args_)))\ngs = g_fn(*args_)\ngs = gs if isinstance(gs, list) or isinstance(gs, tuple) else [gs]\ngs = [x2t(g) for g in gs]\nfor arg, g in zip(args, gs):\narg.grad = torch.detach(g)\nreturn loss\nelse:\nclosure = None\ntp = TablePrinter(\n[\"it\", \"imprv\", \"loss\", \"||g||_2\"],\n[\"%05d\", \"%9.4e\", \"%9.4e\", \"%9.4e\"],\nprefix=verbose_prefix,\nuse_writer=use_writer,\n)\ng_norm = float(\"inf\")\nif verbose:\nprint_fn(tp.make_header())\ntry:\nfor it in rng_wrapper(range(max_it)):\nif USE_TORCH:\nargs_prev = [arg.detach().clone() for arg in args]\nloss = opt.step(closure)\nif full_output:\nargs_hist.append([arg.detach().clone() for arg in args])\nif callback_fn is not None:\ncallback_fn(*[t2j(arg) for arg in args])\nimprv = sum(\ntorch.norm(arg_prev - arg).detach() for (arg, arg_prev) in zip(args, args_prev)\n)\nif verbose or use_writer:\nclosure()\ng_norm = sum(arg.grad.norm().detach() for arg in args if arg.grad is not None)\nimprv, loss = imprv.detach(), loss.detach()\nelse:\narg_prev = arg\narg, opt_state = opt_update_fn(arg, opt_state)\nif full_output:\nargs_hist.append([arg])\nif callback_fn is not None:\ncallback_fn(arg, opt=opt, opt_state=opt_state)\nimprv = jaxm.norm(arg - arg_prev)\nloss = opt_state.value\nif verbose or use_writer:\ng_norm = jaxm.norm(opt_state.grad)\nline = tp.make_values([it, imprv, loss, g_norm])\nif verbose:\nprint_fn(line)\nif imprv &lt; 1e-9:\nbreak\nexcept KeyboardInterrupt:\npass\nif verbose:\nprint_fn(tp.make_footer())\nif USE_TORCH:\nret = [t2j(arg.detach()) for arg in args]\nret = ret if len(args) &gt; 1 else ret[0]\nargs_hist = [[t2j(arg) for arg in z] for z in args_hist]\nargs_hist = [z if len(args) &gt; 1 else z[0] for z in args_hist]\nstate = dict(state, opt=opt)\nelse:\nstate = dict(state, opt=opt, opt_state=opt_state)\nret = arg\nreturn (ret, args_hist, state) if full_output else ret\n</code></pre>"},{"location":"api/sensitivity_jax/extras/optimization/sqp/minimize_sqp/","title":"Minimize sqp","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.extras.optimization.minimize_lbfgs</p>next &gt;&gt;&gt;<p>sensitivity_jax.generate_optimization_fns</p>"},{"location":"api/sensitivity_jax/extras/optimization/sqp/minimize_sqp/#sensitivity_jax.extras.optimization.sqp.minimize_sqp","title":"<code>sensitivity_jax.extras.optimization.sqp.minimize_sqp(f_fn, g_fn, h_fn, *args, reg0=1e-07, verbose=False, verbose_prefix='', max_it=100, ls_pts_nb=5, force_step=False, full_output=False, callback_fn=None, use_writer=False, use_tqdm=False, state=None, parallel_ls=False)</code>","text":"<p>Minimizes an unconstrained objective using Sequential Quadratic Programming (SQP).</p> <p>Parameters:</p> Name Type Description Default <code>f_fn</code> <code>Callable</code> <p>Objective function.</p> required <code>g_fn</code> <code>Callable</code> <p>Gradient function.</p> required <code>h_fn</code> <code>Callable</code> <p>Hessian function.</p> required <code>*args</code> <code>Array</code> <p>Arguments.</p> <code>()</code> <code>reg0</code> <code>float</code> <p>Regularization parameter. Defaults to 1e-7.</p> <code>1e-07</code> <code>verbose</code> <code>bool</code> <p>If True, prints the optimization progress. Defaults to False.</p> <code>False</code> <code>verbose_prefix</code> <code>str</code> <p>Prefix to add to the printed progress. Defaults to \"\".</p> <code>''</code> <code>max_it</code> <code>int</code> <p>Maximum number of iterations. Defaults to 100.</p> <code>100</code> <code>ls_pts_nb</code> <code>int</code> <p>Number of points to use in the line search. Defaults to 5.</p> <code>5</code> <code>force_step</code> <code>bool</code> <p>Forces a step even if the line search fails. Defaults to False.</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>If True, returns additional information. Defaults to False.</p> <code>False</code> <code>callback_fn</code> <code>Callable</code> <p>Ignored.</p> <code>None</code> <code>use_writer</code> <code>bool</code> <p>If True, ues a writer to print the progress. Defaults to False.</p> <code>False</code> <code>use_tqdm</code> <code>Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook]</code> <p>If True, uses tqdm to print the progress. Defaults to False.</p> <code>False</code> <code>state</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary containing the optimization state. Defaults to None.</p> <code>None</code> <code>parallel_ls</code> <code>bool</code> <p>If True, uses parallel line search. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Array | tuple[Array, list[Array], dict[str, Any]]</code> <p>Optimized parameters or a tuple with extra information if <code>full_output</code> is True.</p> Source code in <code>sensitivity_jax/extras/optimization/sqp.py</code> <pre><code>def minimize_sqp(\nf_fn: Callable,\ng_fn: Callable,\nh_fn: Callable,\n*args: Array,\nreg0: float = 1e-7,\nverbose: bool = False,\nverbose_prefix: str = \"\",\nmax_it: int = 100,\nls_pts_nb: int = 5,\nforce_step: bool = False,\nfull_output: bool = False,\ncallback_fn: Callable = None,\nuse_writer: bool = False,\nuse_tqdm: Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook] = False,\nstate: Optional[Dict[str, Any]] = None,\nparallel_ls: bool = False,\n) -&gt; Array | tuple[Array, list[Array], dict[str, Any]]:\n\"\"\"\n    Minimizes an unconstrained objective using Sequential Quadratic Programming (SQP).\n    Args:\n        f_fn (Callable): Objective function.\n        g_fn (Callable): Gradient function.\n        h_fn (Callable): Hessian function.\n        *args (Array): Arguments.\n        reg0 (float, optional): Regularization parameter. Defaults to 1e-7.\n        verbose (bool, optional): If True, prints the optimization progress. Defaults to False.\n        verbose_prefix (str, optional): Prefix to add to the printed progress. Defaults to \"\".\n        max_it (int, optional): Maximum number of iterations. Defaults to 100.\n        ls_pts_nb (int, optional): Number of points to use in the line search. Defaults to 5.\n        force_step (bool, optional): Forces a step even if the line search fails. Defaults to False.\n        full_output (bool, optional): If True, returns additional information. Defaults to False.\n        callback_fn (Callable, optional): Ignored.\n        use_writer (bool, optional): If True, ues a writer to print the progress. Defaults to False.\n        use_tqdm (Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook], optional): If True, uses tqdm to print the progress. Defaults to False.\n        state (Optional[Dict[str, Any]], optional): A dictionary containing the optimization state. Defaults to None.\n        parallel_ls (bool, optional): If True, uses parallel line search. Defaults to False.\n    Returns:\n        Optimized parameters or a tuple with extra information if `full_output` is True.\n    \"\"\"\nstate = state if state is not None else dict()\narg = args[0]\nif \"opt\" not in state:\nopt = SQPSolver(\n#f_fn, g_fn=g_fn, h_fn=h_fn, linesearch=\"scan\", maxls=ls_pts_nb, device=arg.device()\nf_fn, g_fn=g_fn, h_fn=h_fn, linesearch=\"scan\", maxls=ls_pts_nb, force_step=force_step\n)\nelse:\nopt = state[\"opt\"]\nopt_state = opt.init_state(arg) if \"opt_state\" not in state else state[\"opt_state\"]\nif isinstance(use_tqdm, bool):\nif use_tqdm:\nprint_fn, rng_wrapper = tqdm_module.tqdm.write, tqdm_module.tqdm\nelse:\nprint_fn, rng_wrapper = print, lambda x, **kw: x\nelse:\nprint_fn, rng_wrapper = use_tqdm.write, use_tqdm\ntp = TablePrinter(\n[\"it\", \"imprv\", \"loss\", \"reg_it\", \"bet\", \"||g_prev||_2\"],\n[\"%05d\", \"%9.4e\", \"%9.4e\", \"%02d\", \"%9.4e\", \"%9.4e\"],\nprefix=verbose_prefix,\nuse_writer=use_writer,\n)\nx_hist = [arg]\nif verbose:\nprint_fn(tp.make_header())\nline = tp.make_values([0, 0, f_fn(arg), 0, 0.0, jaxm.norm(g_fn(arg))])\nprint_fn(line)\ntry:\nfor it in rng_wrapper(\nrange(max_it), disable=not (use_tqdm if isinstance(use_tqdm, bool) else True)\n):\nnew_arg, opt_state = opt.update(arg, opt_state)\nimprv = jaxm.norm(new_arg - arg)\n#loss = opt_state.best_loss\nloss = f_fn(new_arg)\nif verbose or use_writer:\nline = tp.make_values([it + 1, imprv, loss, 0, 0.0, jaxm.norm(g_fn(new_arg))])\nif verbose:\nprint_fn(line)\narg = new_arg\nx_hist.append(arg)\nif imprv &lt; 1e-9:\nbreak\nexcept (KeyboardInterrupt, InterruptedError):\npass\nif verbose:\nprint_fn(tp.make_footer())\nif full_output:\nreturn opt_state.best_params, x_hist + [opt_state.best_params], state\nelse:\nreturn opt_state.best_params\n</code></pre>"},{"location":"api/sensitivity_jax/sensitivity/generate_optimization_fns/","title":"Generate optimization fns","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.extras.optimization.minimize_sqp</p>next &gt;&gt;&gt;<p>sensitivity_jax.generate_optimization_with_state_fns</p>"},{"location":"api/sensitivity_jax/sensitivity/generate_optimization_fns/#sensitivity_jax.sensitivity.generate_optimization_fns","title":"<code>sensitivity_jax.sensitivity.generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad=False, optimizations=None, jit=True, custom_arg_serializer=None)</code>","text":"<p>Directly generates upper/outer bilevel program derivative functions.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable</code> <p>loss_fn(z, *params), upper/outer level loss</p> required <code>opt_fn</code> <code>Callable</code> <p>opt_fn(*params) = z, lower/inner argmin function</p> required <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>normalize_grad</code> <code>bool</code> <p>whether to normalize the gradient by its norm</p> <code>False</code> <code>jit</code> <code>bool</code> <p>whether to apply just-in-time (jit) compilation to the functions</p> <code>True</code> <p>Returns:</p> Type Description <p><code>f_fn(*params), g_fn(*params), h_fn(*params)</code></p> <p>parameters-only upper/outer level loss, gradient and Hessian.</p> Source code in <code>sensitivity_jax/sensitivity.py</code> <pre><code>def generate_optimization_fns(\nloss_fn: Callable,\nopt_fn: Callable,\nk_fn: Callable,\nnormalize_grad: bool = False,\noptimizations: Mapping = None,\njit: bool = True,\ncustom_arg_serializer: Optional[Callable] = None,\n):\n\"\"\"Directly generates upper/outer bilevel program derivative functions.\n    Args:\n        loss_fn: loss_fn(z, *params), upper/outer level loss\n        opt_fn: opt_fn(*params) = z, lower/inner argmin function\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        normalize_grad: whether to normalize the gradient by its norm\n        jit: whether to apply just-in-time (jit) compilation to the functions\n    Returns:\n        ``f_fn(*params), g_fn(*params), h_fn(*params)``\n        parameters-only upper/outer level loss, gradient and Hessian.\n    \"\"\"\nsol_cache = {}\noptimizations = {} if optimizations is None else copy(optimizations)\n@fn_with_sol_cache(opt_fn, sol_cache, jit=jit, custom_arg_serializer=custom_arg_serializer)\ndef f_fn(z, *params, **nondiff_kw):\nreturn loss_fn(z, *params, **nondiff_kw)\n@fn_with_sol_cache(opt_fn, sol_cache, jit=jit, custom_arg_serializer=custom_arg_serializer)\ndef g_fn(z, *params, **nondiff_kw):\ng = JACOBIAN(loss_fn, argnums=range(len(params) + 1))(z, *params, **nondiff_kw)\nDp = implicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=None if len(nondiff_kw) == 0 else nondiff_kw,\nDg=g[0],\noptimizations=optimizations,\n)\nDp = Dp if len(params) != 1 else [Dp]\nret = [Dp + g for (Dp, g) in zip(Dp, g[1:])]\nif normalize_grad:\nret = [(z / (jaxm.norm(z) + 1e-7)) for z in ret]\nreturn ret[0] if len(ret) == 1 else ret\n@fn_with_sol_cache(opt_fn, sol_cache, jit=jit, custom_arg_serializer=custom_arg_serializer)\ndef h_fn(z, *params, **nondiff_kw):\ng = JACOBIAN(loss_fn, argnums=range(len(params) + 1))(z, *params, **nondiff_kw)\nif optimizations.get(\"Hz_fn\", None) is None:\noptimizations[\"Hz_fn\"] = jaxm.hessian(loss_fn)\nHz_fn = optimizations[\"Hz_fn\"]\nHz = Hz_fn(z, *params, **nondiff_kw)\nH = [Hz] + HESSIAN_DIAG(lambda *params: loss_fn(z, *params, **nondiff_kw))(*params)\n_, Dpp = implicit_hessian(\nk_fn,\nz,\n*params,\nnondiff_kw=None if len(nondiff_kw) == 0 else nondiff_kw,\nDg=g[0],\nHg=H[0],\noptimizations=optimizations,\n)\nDpp = Dpp if len(params) != 1 else [Dpp]\nret = [Dpp + H for (Dpp, H) in zip(Dpp, H[1:])]\nreturn ret[0] if len(ret) == 1 else ret\nreturn (f_fn, g_fn, h_fn)\n</code></pre>"},{"location":"api/sensitivity_jax/sensitivity/generate_optimization_with_state_fns/","title":"Generate optimization with state fns","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.generate_optimization_fns</p>next &gt;&gt;&gt;<p>sensitivity_jax.implicit_hessian</p>"},{"location":"api/sensitivity_jax/sensitivity/generate_optimization_with_state_fns/#sensitivity_jax.sensitivity.generate_optimization_with_state_fns","title":"<code>sensitivity_jax.sensitivity.generate_optimization_with_state_fns(loss_fn, opt_fn, k_fn, normalize_grad=False, optimizations=None, jit=True, custom_arg_serializer=None)</code>","text":"<p>Directly generates upper/outer bilevel program derivative functions.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable</code> <p>loss_fn(z, *params), upper/outer level loss</p> required <code>opt_fn</code> <code>Callable</code> <p>opt_fn(*params) = z, lower/inner argmin function</p> required <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>normalize_grad</code> <code>bool</code> <p>whether to normalize the gradient by its norm</p> <code>False</code> <code>jit</code> <code>bool</code> <p>whether to apply just-in-time (jit) compilation to the functions</p> <code>True</code> <p>Returns:</p> Type Description <p><code>f_fn(*params), g_fn(*params), h_fn(*params)</code></p> <p>parameters-only upper/outer level loss, gradient and Hessian.</p> Source code in <code>sensitivity_jax/sensitivity.py</code> <pre><code>def generate_optimization_with_state_fns(\nloss_fn: Callable,\nopt_fn: Callable,\nk_fn: Callable,\nnormalize_grad: bool = False,\noptimizations: Mapping = None,\njit: bool = True,\ncustom_arg_serializer: Optional[Callable] = None,\n):\n\"\"\"Directly generates upper/outer bilevel program derivative functions.\n    Args:\n        loss_fn: loss_fn(z, *params), upper/outer level loss\n        opt_fn: opt_fn(*params) = z, lower/inner argmin function\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        normalize_grad: whether to normalize the gradient by its norm\n        jit: whether to apply just-in-time (jit) compilation to the functions\n    Returns:\n        ``f_fn(*params), g_fn(*params), h_fn(*params)``\n        parameters-only upper/outer level loss, gradient and Hessian.\n    \"\"\"\nsol_cache = {}\noptimizations = {} if optimizations is None else copy(optimizations)\n@fn_with_sol_and_state_cache(\nopt_fn, sol_cache, jit=jit, custom_arg_serializer=custom_arg_serializer\n)\ndef f_fn(z, *params, **nondiff_kw):\nreturn loss_fn(z, *params, **nondiff_kw)\n@fn_with_sol_and_state_cache(\nopt_fn, sol_cache, jit=jit, custom_arg_serializer=custom_arg_serializer\n)\ndef g_fn(z, *params, **nondiff_kw):\ng = JACOBIAN(loss_fn, argnums=range(len(params) + 1))(z, *params, **nondiff_kw)\nDp = implicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=None if len(nondiff_kw) == 0 else nondiff_kw,\nDg=g[0],\noptimizations=optimizations,\n)\nDp = Dp if len(params) != 1 else [Dp]\nret = [Dp + g for (Dp, g) in zip(Dp, g[1:])]\nif normalize_grad:\nret = [(z / (jaxm.norm(z) + 1e-7)) for z in ret]\nreturn ret[0] if len(ret) == 1 else ret\n@fn_with_sol_and_state_cache(\nopt_fn, sol_cache, jit=jit, custom_arg_serializer=custom_arg_serializer\n)\ndef h_fn(z, *params, **nondiff_kw):\ng = JACOBIAN(loss_fn, argnums=range(len(params) + 1))(z, *params, **nondiff_kw)\nif optimizations.get(\"Hz_fn\", None) is None:\noptimizations[\"Hz_fn\"] = jaxm.hessian(loss_fn)\nHz_fn = optimizations[\"Hz_fn\"]\nHz = Hz_fn(z, *params, **nondiff_kw)\nH = [Hz] + HESSIAN_DIAG(lambda *params: loss_fn(z, *params, **nondiff_kw))(*params)\n_, Dpp = implicit_hessian(\nk_fn,\nz,\n*params,\nnondiff_kw=None if len(nondiff_kw) == 0 else nondiff_kw,\nDg=g[0],\nHg=H[0],\noptimizations=optimizations,\n)\nDpp = Dpp if len(params) != 1 else [Dpp]\nret = [Dpp + H for (Dpp, H) in zip(Dpp, H[1:])]\nreturn ret[0] if len(ret) == 1 else ret\nreturn (f_fn, g_fn, h_fn)\n</code></pre>"},{"location":"api/sensitivity_jax/sensitivity/implicit_hessian/","title":"Implicit hessian","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.generate_optimization_with_state_fns</p>next &gt;&gt;&gt;<p>sensitivity_jax.implicit_jacobian</p>"},{"location":"api/sensitivity_jax/sensitivity/implicit_hessian/#sensitivity_jax.sensitivity.implicit_hessian","title":"<code>sensitivity_jax.sensitivity.implicit_hessian(k_fn, z, *params, nondiff_kw=None, Dg=None, Hg=None, jvp_vec=None, optimizations=None)</code>","text":"<p>Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>JAXArray</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>JAXArray</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>nondiff_kw</code> <code>Mapping</code> <p>nondifferentiable parameters to the implicit function</p> <code>None</code> <code>Dg</code> <code>JAXArray</code> <p>gradient sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>Hg</code> <code>JAXArray</code> <p>Hessian sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>jvp_vec</code> <code>Union[JAXArray, Sequence[JAXArray]]</code> <p>right sensitivity vector(s) (wrt p) for Hessian-vector-product</p> <code>None</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Hessian/chain rule Hessian as specified by arguments</p> Source code in <code>sensitivity_jax/sensitivity.py</code> <pre><code>def implicit_hessian(\nk_fn: Callable,\nz: JAXArray,\n*params: JAXArray,\nnondiff_kw: Mapping = None,\nDg: JAXArray = None,\nHg: JAXArray = None,\njvp_vec: Union[JAXArray, Sequence[JAXArray]] = None,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        nondiff_kw: nondifferentiable parameters to the implicit function\n        Dg: gradient sensitivity vector (wrt z), for chain rule\n        Hg: Hessian sensitivity vector (wrt z), for chain rule\n        jvp_vec: right sensitivity vector(s) (wrt p) for Hessian-vector-product\n        optimizations: optional optimizations\n    Returns:\n        Hessian/chain rule Hessian as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nnondiff_kw = {} if nondiff_kw is None else nondiff_kw\nzlen, plen = prod(z.shape), [prod(param.shape) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\nif jvp_vec is not None:\nassert Dg is not None\nif nondiff_kw is not None:\nk_fn_ = lambda z, *params: k_fn(z, *params, **nondiff_kw)\nelse:\nk_fn_ = k_fn\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn_)\n#############################################################\n# compute 2nd implicit gradients\nif Dg is not None:\nassert Dg.size == zlen\nassert Hg is None or Hg.size == zlen**2\nDg_ = Dg.reshape((zlen, 1))\nHg_ = Hg.reshape((zlen, zlen)) if Hg is not None else Hg\n# compute the left hand vector in the VJP\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(z, *params, rhs=Dg_.reshape((zlen, 1)), T=True)\nfn = lambda z, *params: jaxm.sum(v.reshape(zlen) * k_fn_(z, *params).reshape(zlen))\nif jvp_vec is not None:\nDpz_jvp = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\njvp_vec=jvp_vec,\noptimizations=optimizations,\n)\n)\nDpz_jvp = [Dpz_jvp.reshape(-1) for Dpz_jvp in Dpz_jvp]\n# compute the 2nd order derivatives consisting of 4 terms\n# term 1 ##############################\ndfn_params = jaxm.grad(lambda *params: fn(z, *params), argnums=range(len(params)))\nDpp1 = _ensure_list(jaxm.jvp(dfn_params, params, tuple(jvp_vec))[1])\nDpp1 = [Dpp1.reshape(plen) for (Dpp1, plen) in zip(Dpp1, plen)]\n# term 2 ##############################\nDpp2 = [\njaxm.jvp(\nlambda z: jaxm.grad(fn, argnums=i + 1)(z, *params),\n(z,),\n(Dpz_jvp.reshape(z.shape),),\n)[1].reshape(plen)\nfor (i, (Dpz_jvp, plen)) in enumerate(zip(Dpz_jvp, plen))\n]\n# term 3 ##############################\ng_ = _ensure_list(\njaxm.jvp(\nlambda *params: jaxm.grad(fn)(z, *params),\nparams,\ntuple(jvp_vec),\n)[1]\n)\nDpp3 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\nDg=g_,\noptimizations=optimizations,\n)\n)[i].reshape(-1)\nfor (i, g_) in enumerate(g_)\n]\n# term 4 ##############################\ng_ = [\njaxm.jvp(\nlambda z: jaxm.grad(fn)(z, *params),\n(z,),\n(Dpz_jvp.reshape(z.shape),),\n)[1]\nfor (i, Dpz_jvp) in enumerate(Dpz_jvp)\n]\nif Hg is not None:\ng_ = [\ng_.reshape(zlen) + Hg_ @ Dpz_jvp.reshape(zlen)\nfor (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n]\nDpp4 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nDg=g_,\nnondiff_kw=nondiff_kw,\noptimizations=optimizations,\n)\n)[i].reshape(plen)\nfor ((i, g_), plen) in zip(enumerate(g_), plen)\n]\nDp = [Dg_.reshape((1, zlen)) @ Dpz_jvp.reshape(zlen) for Dpz_jvp in Dpz_jvp]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [Dp.reshape(()) for Dp in Dp]\nDpp_shaped = [Dpp.reshape(param.shape) for (Dpp, param) in zip(Dpp, params)]\nelse:\n# compute the full first order 1st gradients\nDpz = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\noptimizations=optimizations,\n)\n)\nDpz = [Dpz.reshape((zlen, plen)) for (Dpz, plen) in zip(Dpz, plen)]\n# compute the 2nd order derivatives consisting of 4 terms\nDpp1 = HESSIAN_DIAG(lambda *params: fn(z, *params))(*params)\nDpp1 = [Dpp1.reshape((plen, plen)) for (Dpp1, plen) in zip(Dpp1, plen)]\ntemp = JACOBIAN(\nlambda *params: JACOBIAN(fn)(z, *params),\nargnums=range(len(params)),\n)(*params)\ntemp = [jaxm.t(temp.reshape((zlen, plen))) for (temp, plen) in zip(temp, plen)]\nDpp2 = [\n(temp @ Dpz).reshape((plen, plen)) for (temp, Dpz, plen) in zip(temp, Dpz, plen)\n]\nDpp3 = [jaxm.t(Dpp2) for Dpp2 in Dpp2]\nDzz = HESSIAN(lambda z: fn(z, *params))(z).reshape((zlen, zlen))\nif Hg is not None:\nDpp4 = [jaxm.t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\nelse:\nDpp4 = [jaxm.t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\nDp = [Dg_.reshape((1, zlen)) @ Dpz for Dpz in Dpz]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\nDpp_shaped = [\nDpp.reshape(param.shape + param.shape) for (Dpp, param) in zip(Dpp, params)\n]\nreturn (Dp_shaped[0], Dpp_shaped[0]) if len(params) == 1 else (Dp_shaped, Dpp_shaped)\nelse:\nDpz, optimizations = implicit_jacobian(\nk_fn,\nz,\n*params,\nnondiff_kw=nondiff_kw,\nfull_output=True,\noptimizations=optimizations,\n)\nDpz = _ensure_list(Dpz)\nDpz = [Dpz.reshape(zlen, plen) for (Dpz, plen) in zip(Dpz, plen)]\n# compute derivatives\nif optimizations.get(\"Dzzk\", None) is None:\nHk = HESSIAN_DIAG(k_fn_)(z, *params)\nDzzk, Dppk = Hk[0], Hk[1:]\noptimizations[\"Dzzk\"] = Dzzk\nelse:\nDppk = HESSIAN_DIAG(lambda *params: k_fn_(z, *params))(*params)\nDzpk = JACOBIAN(\nlambda *params: JACOBIAN(k_fn_)(z, *params),\nargnums=range(len(params)),\n)(*params)\nDppk = [Dppk.reshape((zlen, plen, plen)) for (Dppk, plen) in zip(Dppk, plen)]\nDzzk = Dzzk.reshape((zlen, zlen, zlen))\nDzpk = [Dzpk.reshape((zlen, zlen, plen)) for (Dzpk, plen) in zip(Dzpk, plen)]\nDpzk = [jaxm.t(Dzpk) for Dzpk in Dzpk]\n# solve the IFT equation\nlhs = [\nDppk\n+ Dpzk @ Dpz[None, ...]\n+ jaxm.t(Dpz)[None, ...] @ Dzpk\n+ (jaxm.t(Dpz)[None, ...] @ Dzzk) @ Dpz[None, ...]\nfor (Dpz, Dzpk, Dpzk, Dppk) in zip(Dpz, Dzpk, Dpzk, Dppk)\n]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDppz = [\n-Dzk_solve_fn(z, *params, rhs=lhs.reshape((zlen, plen * plen)), T=False).reshape(\n(zlen, plen, plen)\n)\nfor (lhs, plen) in zip(lhs, plen)\n]\n# return computed values\nDpz_shaped = [Dpz.reshape(z.shape + param.shape) for (Dpz, param) in zip(Dpz, params)]\nDppz_shaped = [\nDppz.reshape(z.shape + param.shape + param.shape) for (Dppz, param) in zip(Dppz, params)\n]\nreturn (Dpz_shaped[0], Dppz_shaped[0]) if len(params) == 1 else (Dpz_shaped, Dppz_shaped)\n</code></pre>"},{"location":"api/sensitivity_jax/sensitivity/implicit_jacobian/","title":"Implicit jacobian","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.implicit_hessian</p>next &gt;&gt;&gt;<p>sensitivity_jax.fn_with_sol_and_state_cache</p>"},{"location":"api/sensitivity_jax/sensitivity/implicit_jacobian/#sensitivity_jax.sensitivity.implicit_jacobian","title":"<code>sensitivity_jax.sensitivity.implicit_jacobian(k_fn, z, *params, nondiff_kw=None, Dg=None, jvp_vec=None, matrix_free_inverse=False, full_output=False, optimizations=None)</code>","text":"<p>Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>JAXArray</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>JAXArray</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>nondiff_kw</code> <code>Mapping</code> <p>nondifferentiable parameters to the implicit function</p> <code>None</code> <code>Dg</code> <code>JAXArray</code> <p>left sensitivity vector (wrt z), for a VJP</p> <code>None</code> <code>jvp_vec</code> <code>Union[JAXArray, Sequence[JAXArray]]</code> <p>right sensitivity vector(s) (wrt p) for a JVP</p> <code>None</code> <code>matrix_free_inverse</code> <code>bool</code> <p>whether to use approximate matrix inversion</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to append accumulated optimizations to the output</p> <code>False</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Jacobian/VJP/JVP as specified by arguments</p> Source code in <code>sensitivity_jax/sensitivity.py</code> <pre><code>def implicit_jacobian(\nk_fn: Callable,\nz: JAXArray,\n*params: JAXArray,\nnondiff_kw: Mapping = None,\nDg: JAXArray = None,\njvp_vec: Union[JAXArray, Sequence[JAXArray]] = None,\nmatrix_free_inverse: bool = False,\nfull_output: bool = False,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        nondiff_kw: nondifferentiable parameters to the implicit function\n        Dg: left sensitivity vector (wrt z), for a VJP\n        jvp_vec: right sensitivity vector(s) (wrt p) for a JVP\n        matrix_free_inverse: whether to use approximate matrix inversion\n        full_output: whether to append accumulated optimizations to the output\n        optimizations: optional optimizations\n    Returns:\n        Jacobian/VJP/JVP as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nzlen, plen = prod(z.shape), [prod(param.shape) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\nif nondiff_kw is not None:\nk_fn_ = lambda z, *params: k_fn(z, *params, **nondiff_kw)\nelse:\nk_fn_ = k_fn\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn_)\n#############################################################\nif Dg is not None:\nif matrix_free_inverse:\nA_fn = lambda x: JACOBIAN(\nlambda z: jaxm.sum(k_fn_(z, *params).reshape(-1) * x.reshape(-1))\n)(z).reshape(x.shape)\nv = -solve_gmres(A_fn, Dg.reshape((zlen, 1)), max_it=300)\nelse:\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(z, *params, rhs=Dg.reshape((zlen, 1)), T=True)\nfn = lambda *params: jaxm.sum(v.reshape(zlen) * k_fn_(z, *params).reshape(zlen))\nDp = JACOBIAN(fn, argnums=range(len(params)))(*params)\nDp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\nret = Dp_shaped[0] if len(params) == 1 else Dp_shaped\nelse:\nif jvp_vec is not None:\nfn = lambda *params: k_fn_(z, *params)\nDp = _ensure_list(jaxm.jvp(fn, params, tuple(jvp_vec))[1])\nDp = [Dp.reshape((zlen, 1)) for (Dp, plen) in zip(Dp, plen)]\nDpk = Dp\nelse:\nDpk = JACOBIAN(\nlambda *params: k_fn_(z, *params),\nargnums=range(len(params)),\n)(*params)\nDpk = [Dpk.reshape((zlen, plen)) for (Dpk, plen) in zip(Dpk, plen)]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDpz = [-Dzk_solve_fn(z, *params, rhs=Dpk, T=False) for Dpk in Dpk]\nif jvp_vec is not None:\nDpz_shaped = [Dpz.reshape(z.shape) for Dpz in Dpz]\nelse:\nDpz_shaped = [Dpz.reshape(z.shape + param.shape) for (Dpz, param) in zip(Dpz, params)]\nret = Dpz_shaped if len(params) != 1 else Dpz_shaped[0]\nreturn (ret, optimizations) if full_output else ret\n</code></pre>"},{"location":"api/sensitivity_jax/utils/fn_with_sol_and_state_cache/","title":"Fn with sol and state cache","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.implicit_jacobian</p>next &gt;&gt;&gt;<p>sensitivity_jax.fn_with_sol_cache</p>"},{"location":"api/sensitivity_jax/utils/fn_with_sol_and_state_cache/#sensitivity_jax.utils.fn_with_sol_and_state_cache","title":"<code>sensitivity_jax.utils.fn_with_sol_and_state_cache(fwd_fn, cache=None, jit=True, use_cache=True, kw_in_key=True, custom_arg_serializer=None)</code>","text":"<p>Wraps a function in a version where computation of the first argument via fwd_fn is cached.</p> <p>Parameters:</p> Name Type Description Default <code>fwd_fn</code> <code>Callable</code> <p>The forward function to hide.</p> required <code>cache</code> <code>Optional[Dict]</code> <p>The cache to (re-)use.</p> <code>None</code> <code>jit</code> <code>bool</code> <p>Whether to jit the forward function. Defaults to True.</p> <code>True</code> <code>use_cache</code> <code>bool</code> <p>Whether to use the cache at all. Defaults to True.</p> <code>True</code> <code>kw_in_key(bool,</code> <code>optional</code> <p>Whether to use keyword arguments in key. Defaults to True.</p> required Source code in <code>sensitivity_jax/utils.py</code> <pre><code>def fn_with_sol_and_state_cache(\nfwd_fn: Callable,\ncache: Optional[Dict] = None,\njit: bool = True,\nuse_cache: bool = True,\nkw_in_key: bool = True,\ncustom_arg_serializer: Optional[Callable] = None,\n):\n\"\"\"Wraps a function in a version where computation of the first argument via fwd_fn is cached.\n    Args:\n        fwd_fn (Callable): The forward function to hide.\n        cache (Optional[Dict], optional): The cache to (re-)use.\n        jit (bool, optional): Whether to jit the forward function. Defaults to True.\n        use_cache (bool, optional): Whether to use the cache at all. Defaults to True.\n        kw_in_key(bool, optional): Whether to use keyword arguments in key. Defaults to True.\n    \"\"\"\ndef inner_decorator(fn):\nnonlocal cache\ncache = cache if cache is None else dict()\nfwd_fn_ = fwd_fn  # assume already jit-ed\ndef fn_with_sol(*args, **kw):\ncache = fn_with_sol.cache\nif custom_arg_serializer is None:\nsol_key = to_tuple_with_kw(*args, **kw) if kw_in_key else to_tuple(*args)\nelse:\nsol_key = (\ncustom_arg_serializer(*args, **kw)\nif kw_in_key\nelse custom_arg_serializer(*args)\n)\nsol, state = fwd_fn_(*args, **kw) if sol_key not in cache else cache[sol_key]\nif use_cache:\ncache.setdefault(sol_key, (sol, state))\nret = fn_with_sol.fn(sol, *args, state=state, **kw)\nreturn ret\nfn_with_sol.cache = cache\nfn_with_sol.fn = jaxm.jit(fn) if jit else fn\nreturn fn_with_sol\nreturn inner_decorator\n</code></pre>"},{"location":"api/sensitivity_jax/utils/fn_with_sol_cache/","title":"Fn with sol cache","text":"&lt;&lt;&lt; prev<p>sensitivity_jax.fn_with_sol_and_state_cache</p>"},{"location":"api/sensitivity_jax/utils/fn_with_sol_cache/#sensitivity_jax.utils.fn_with_sol_cache","title":"<code>sensitivity_jax.utils.fn_with_sol_cache(fwd_fn, cache=None, jit=True, use_cache=True, kw_in_key=True, custom_arg_serializer=None)</code>","text":"<p>Wraps a function in a version where computation of the first argument via fwd_fn is cached.</p> <p>Parameters:</p> Name Type Description Default <code>fwd_fn</code> <code>Callable</code> <p>The forward function to hide.</p> required <code>cache</code> <code>Optional[Dict]</code> <p>The cache to (re-)use.</p> <code>None</code> <code>jit</code> <code>bool</code> <p>Whether to jit the forward function. Defaults to True.</p> <code>True</code> <code>use_cache</code> <code>bool</code> <p>Whether to use the cache at all. Defaults to True.</p> <code>True</code> <code>kw_in_key(bool,</code> <code>optional</code> <p>Whether to use keyword arguments in key. Defaults to True.</p> required Source code in <code>sensitivity_jax/utils.py</code> <pre><code>def fn_with_sol_cache(\nfwd_fn: Callable,\ncache: Optional[Dict] = None,\njit: bool = True,\nuse_cache: bool = True,\nkw_in_key: bool = True,\ncustom_arg_serializer: Optional[Callable] = None,\n):\n\"\"\"Wraps a function in a version where computation of the first argument via fwd_fn is cached.\n    Args:\n        fwd_fn (Callable): The forward function to hide.\n        cache (Optional[Dict], optional): The cache to (re-)use.\n        jit (bool, optional): Whether to jit the forward function. Defaults to True.\n        use_cache (bool, optional): Whether to use the cache at all. Defaults to True.\n        kw_in_key(bool, optional): Whether to use keyword arguments in key. Defaults to True.\n    \"\"\"\ndef retrieve_solution(*args, **kw):\nself = retrieve_solution\nsol_key = to_tuple_with_kw(*args, **kw) if kw_in_key else to_tuple(*args)\nif custom_arg_serializer is None:\nsol_key = to_tuple_with_kw(*args, **kw) if kw_in_key else to_tuple(*args)\nelse:\nsol_key = (\ncustom_arg_serializer(*args, **kw)\nif kw_in_key\nelse custom_arg_serializer(*args)\n)\nsol = fwd_fn(*args, **kw) if sol_key not in self.cache else self.cache[sol_key]\nsol = jax.device_put(sol, jax.devices(\"cpu\")[0])\nif use_cache:\nself.cache.setdefault(sol_key, sol)\nreturn sol\ndef inner_decorator(fn):\nnonlocal cache\ncache = cache if cache is None else dict()\nretrieve_solution.cache = cache\nfwd_fn_ = fwd_fn  # assume already jit-ed\ndef fn_with_sol(*args, **kw):\n#cache = fn_with_sol.cache\n#if custom_arg_serializer is None:\n#    sol_key = to_tuple_with_kw(*args, **kw) if kw_in_key else to_tuple(*args)\n#else:\n#    sol_key = (\n#        custom_arg_serializer(*args, **kw)\n#        if kw_in_key\n#        else custom_arg_serializer(*args)\n#    )\n#sol = fwd_fn_(*args, **kw) if sol_key not in cache else cache[sol_key]\nsol_shape_dtype = jax.eval_shape(fwd_fn_, *args, **kw)\nsol = jax.pure_callback(retrieve_solution, sol_shape_dtype, *args, **kw)\n#ret = fn_with_sol.fn(sol, *args, **kw)\nret = fn(sol, *args, **kw)\nreturn ret\nfn_with_sol.cache = cache\n#fn_with_sol.fn = jaxm.jit(fn) if jit else fn\nfn_with_sol = jaxm.jit(fn_with_sol) if jit else fn_with_sol\nreturn fn_with_sol\nreturn inner_decorator\n</code></pre>"}]}